AT_BANNER([ovn multinode system tests using ovn-fake-multinode])

AT_SETUP([ovn multinode basic test])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Add ACLs to drop all traffic
check multinode_nbctl pg-add pg0 sw0-port1 sw0-port2
check multinode_nbctl acl-add pg0 to-lport 1001 "outport == @pg0 && ip4" drop
check multinode_nbctl --wait=sb sync

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4], \
[1], [ignore])

# Add ACLs to allow icmp traffic
check multinode_nbctl acl-add pg0 to-lport 1002 "outport == @pg0 && ip4 && icmp" allow-related
check multinode_nbctl --wait=sb sync

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::4/64 1000::a

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

check multinode_nbctl lsp-set-addresses sw1-port1 unknown
m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - distributed router - geneve])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add-localnet-port public ln-public public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add-router-port public public-lr0 lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

# create LB
check multinode_nbctl lb-add lb0 10.0.0.1:8080 10.0.0.4:8080 udp
check multinode_nbctl ls-lb-add sw0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw0p2], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the geneve tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1400 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping6 -c 5 -s 1450 -M do 2000::3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1000])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 10 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1000"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q 'mtu 942'])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - distributed router - vxlan])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset vxlan tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=vxlan
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q vxlan_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add-localnet-port public ln-public public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add-router-port public public-lr0 lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the vxlan tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 |grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1150"])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw_router_port - geneve])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add-localnet-port public ln-public public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add-router-port public public-lr0 lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

check multinode_nbctl lrp-set-gateway-chassis lr0-sw0 ovn-chassis-1 10
check multinode_nbctl lrp-set-gateway-chassis lr0-sw1 ovn-chassis-2 10

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1400 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping6 -c 5 -s 1450 -M do 2000::3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1100"])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw_router_port - vxlan])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=vxlan
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q vxlan_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add-localnet-port public ln-public public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add-router-port public public-lr0 lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

check multinode_nbctl lrp-set-gateway-chassis lr0-sw0 ovn-chassis-1 10
check multinode_nbctl lrp-set-gateway-chassis lr0-sw1 ovn-chassis-2 10

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1150"])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw router - geneve])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0 -- set Logical_Router lr0 options:chassis=ovn-gw-1
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add-localnet-port public ln-public public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add-router-port public public-lr0 lr0-public
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

# create LB
check multinode_nbctl lb-add lb0 10.0.0.1:8080 20.0.0.3:8080 udp
check multinode_nbctl lr-lb-add lr0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw1p1], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1400 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping6 -c 5 -s 1450 -M do 2000::3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 | grep -q "mtu = 1100"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q 'mtu 942'])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw router - vxlan])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=vxlan
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q vxlan_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0 -- set Logical_Router lr0 options:chassis=ovn-gw-1
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add-localnet-port public ln-public public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add-router-port public public-lr0 lr0-public
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

# create LB
check multinode_nbctl lb-add lb0 10.0.0.1:8080 20.0.0.3:8080 udp
check multinode_nbctl lr-lb-add lr0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw1p1], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 | grep -q "mtu = 1150"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q 'mtu 950'])

AT_CLEANUP

m4_define([PMTUD_SWITCH_TESTS],
  [
    AT_SETUP([ovn multinode pmtu - logical switch - $1])
    encap=$1
    if test "$encap" = "vxlan"; then
      encap_sys="vxlan_sys"
      overhead=50
    else
      encap_sys="genev_sys"
      overhead=58
    fi

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=$encap
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q $encap_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q $encap_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q $encap_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

check multinode_nbctl lrp-set-gateway-chassis lr0-sw0 ovn-chassis-1 10
check multinode_nbctl lrp-set-gateway-chassis lr0-sw1 ovn-chassis-2 10

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

check multinode_nbctl lb-add lb0 10.0.0.1:8080 10.0.0.4:8080 udp
check multinode_nbctl ls-lb-add sw0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw0p2], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the geneve tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 10.0.0.4 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the geneve tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1100 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
mtu=$((1000 - overhead))
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q "mtu $mtu"])

# Reset back to geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

AT_CLEANUP

AT_SETUP([ovn multinode NAT on a provider network with no localnet ports])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add-router-port sw0 sw0-lr0 lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add-router-port sw1 sw1-lr0 lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add-localnet-port public ln-public public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add-router-port public public-lr0 lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10

check multinode_nbctl lr-nat-add lr0 dnat_and_snat 172.20.0.110 10.0.0.3 sw0-port1 30:54:00:00:00:03
check multinode_nbctl lr-nat-add lr0 dnat_and_snat 172.20.0.120 20.0.0.3
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# Create a logical port pub-p1 and bind it in ovn-chassis-1
check multinode_nbctl lsp-add public public-port1
check multinode_nbctl lsp-set-addresses public-port1 "60:54:00:00:00:03 172.168.0.50"

m_as ovn-chassis-1 /data/create_fake_vm.sh public-port1 pubp1 60:54:00:00:00:03 172.20.0.50 24 172.20.0.100

check multinode_nbctl --wait=hv sync

# First do basic ping tests before deleting the localnet port - ln-public.
# Once the localnet port is deleted from public ls, routing for 172.20.0.0/24
# is centralized on ovn-gw-1.

# This function checks the North-South traffic.
run_ns_traffic() {
  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [arp -d 172.20.0.110], [ignore], [ignore])
  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [arp -d 172.20.0.120], [ignore], [ignore])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.100 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.110 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.120 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.50 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-2], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.50 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  # Now ping from pubp1 to 172.20.0.100, 172.20.0.110, 172.20.0.120, 10.0.0.3 and 20.0.0.3
  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.100 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.110 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.120 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
}

# Test out the N-S traffic.
run_ns_traffic

# Delete the localnet port by changing the type of ln-public to VIF port.
check multinode_nbctl --wait=hv lsp-set-type ln-public ""

# cr-port should not be created for public-lr0 since the option
# centralize_routing=true is not yet set for lr0-public.
m_check_row_count Port_Binding 0 logical_port=cr-public-lr0

# Set the option - centralize_routing now.
check multinode_nbctl --wait=hv set logical_router_port lr0-public options:centralize_routing=true

m_check_row_count Port_Binding 1 logical_port=cr-public-lr0
m_check_column chassisredirect Port_Binding type logical_port=cr-public-lr0

# Test out the N-S traffic.
run_ns_traffic

# Re-add the localnet port
check multinode_nbctl --wait=hv lsp-set-type ln-public localnet

m_check_row_count Port_Binding 0 logical_port=cr-public-lr0

# Test out the N-S traffic.
run_ns_traffic

# Delete the ln-public port this time.
check multinode_nbctl --wait=hv lsp-del ln-public

m_check_row_count Port_Binding 1 logical_port=cr-public-lr0
m_check_column chassisredirect Port_Binding type logical_port=cr-public-lr0

# Test out the N-S traffic.
run_ns_traffic

AT_CLEANUP
])

PMTUD_SWITCH_TESTS(["geneve"])
PMTUD_SWITCH_TESTS(["vxlan"])

AT_SETUP([Migration of container ports])
# Migrate vif port between chassis-1 and chassis-3; send packets between
# chassis-2 and chassis-1/chassis-3, and check that
# - packet handing on src works before migration.
# - packet handing on src & dst works during migration.
# - packet handing on dst works after migration.
# Do the same for container ports.
# The container port migration is tested in two different orders,
# setting iface-id on dst resp. before and after requested-chassis.

# Check that ovn-fake-multinode setup is up and running
# check_fake_multinode_setup
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources
m_as ovn-chassis-3 ovs-vsctl del-br br-int
m_as ovn-chassis-3 ip --all netns delete

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-3 ip link show | grep -q genev_sys])

# check_ping(src_port, src_chassis, dst_port, expected_status)
# Check whether ping, from src_port on src_chassis to dst_port works as expected.
# expected_status:
#   - "success" (or empty): ping should succeed between src and dst.
#   - "potential-duplicates": ping should succeed, but we might have potential duplicate packets.
#   - "lost": ping should fail, no packets should go through.
# Source ip namespace is derived from previous parameters:
#   - parent_port name for container ports.
#   - same name as port itself for other ports.
# Dest ip is retrieved from dst_port.
check_ping() {
    src_port=$1
    src_chassis=$2
    dst_port=$3
    status=${4:-success}

    parent_port=$(multinode_sbctl get port_binding $src_port parent_port)
    if [[ "$parent_port" != "[]" ]]; then
        src_ns=$parent_port
    else
        src_ns=$src_port
    fi
    dst_chassis_uuid=$(multinode_sbctl get port_binding $dst_port chassis)
    requested_chassis_uuid=$(multinode_sbctl get port_binding $dst_port requested_chassis)
    dst_chassis=$(multinode_sbctl --bare --columns name list chassis $dst_chassis_uuid)
    dst_ip=$(multinode_nbctl lsp-get-addresses $dst_port | awk '{print $2}')
    echo "$src_port on $src_chassis => $dst_port on $dst_chassis(requested_chassis=$requested_chassis_uuid)"
    M_NS_CHECK_EXEC([$src_chassis], [$src_ns], [ping -q -c 3 -i 0.1 -w 2 $dst_ip | FORMAT_PING], \
[0], [stdout])
    if [[ "$status" == "success" ]]; then
        AT_CHECK([cat stdout | grep -c "3 packets transmitted, 3 received, 0% packet loss"], [0], [dnl
1
])
    elif [[ "$status" == "potential-duplicates" ]]; then
        AT_CHECK([cat stdout | grep "3 packets transmitted" | grep -c "3 received"], [0], [dnl
1
])
    elif [[ "$status" == "lost" ]]; then
        AT_CHECK([cat stdout | grep -c "100% packet loss"], [0], [dnl
1
])
    else
        echo "unexpected status $status"
        AT_FAIL_IF([:])
    fi
}

check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 migrator
check multinode_nbctl lsp-set-addresses migrator "50:54:00:00:00:09 10.0.0.9 1000::9"
for i in 1 2 3; do
    check multinode_nbctl lsp-add sw0 sw0-port${i}
    check multinode_nbctl lsp-set-addresses sw0-port${i} "50:54:00:00:00:0${i} 10.0.0.${i} 1000::${i}"
done

# Set requested chassis before creating migrator on chassis-3
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1

m_as ovn-chassis-1 /data/create_fake_vm.sh migrator migrator 50:54:00:00:00:09 10.0.0.9 24 10.0.0.10 1000::9/64 1000::a
m_as ovn-chassis-3 /data/create_fake_vm.sh migrator migrator 50:54:00:00:00:09 10.0.0.9 24 10.0.0.10 1000::9/64 1000::a
for i in 1 2 3; do
    m_as ovn-chassis-${i} /data/create_fake_vm.sh sw0-port${i} sw0-port${i} 50:54:00:00:00:0${i} 10.0.0.${i} 24 10.0.0.10 1000::${i}/64 1000::a
done

m_wait_for_ports_up

M_START_TCPDUMP([ovn-chassis-1], [-neei genev_sys_6081 arp or ip], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-neei migrator-p arp or ip], [ch1_migrator])
M_START_TCPDUMP([ovn-chassis-2], [-neei genev_sys_6081 arp or ip], [ch2_genev])
for i in 1 2 3; do
    M_START_TCPDUMP([ovn-chassis-${i}], [-neei sw0-port${i}-p arp or ip], [ch${i}_sw0-port${i}])
done
M_START_TCPDUMP([ovn-chassis-3], [-neei genev_sys_6081 arp or ip], [ch3_genev])
M_START_TCPDUMP([ovn-chassis-3], [-neei migrator-p arp or ip], [ch3_migrator])

AS_BOX([Migration with vifs])
for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
done

echo "== Starting migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1,ovn-chassis-3

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i}
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator "potential-duplicates"
done

echo "== Finalizing migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-3

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
done

AS_BOX([Migration with container ports])
# Create container ports.
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 mig-cont migrator 10 \
                   -- lsp-set-addresses mig-cont "f0:00:00:01:02:09 20.0.0.9"
for i in 1 2 3; do
    check multinode_nbctl lsp-add sw1 cont${i} sw0-port${i} 10 \
                       -- lsp-set-addresses cont${i} "f0:00:00:01:02:0${i} 20.0.0.${i}"
done

# Create the interface for lport mig-cont
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip link add link migrator name cont type vlan id 10], [0])
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip link set cont address f0:00:00:01:02:09], [0])
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip link set cont up], [0])
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip addr add 20.0.0.9/24 dev cont], [0])

M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip link add link migrator name cont type vlan id 10], [0])
M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip link set cont address f0:00:00:01:02:09], [0])
M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip link set cont up], [0])
M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip addr add 20.0.0.9/24 dev cont], [0])

# Create the cont interface for lport sw0-port1, sw0-port2, sw0-port3
for i in 1 2 3; do
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip link add link sw0-port${i} name cont${i} type vlan id 10], [0])
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip link set cont${i} address f0:00:00:01:02:0${i}], [0])
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip link set cont${i} up], [0])
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip addr add 20.0.0.${i}/24 dev cont${i}], [0])
done

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator
done

echo "== Starting migration back =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-3,ovn-chassis-1

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping mig-cont ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont "potential-duplicates"
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator "potential-duplicates"
done

echo "== Finalizing migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping migrator ovn-chassis-3 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator
done

echo "== Starting another migration, this time before starting dst VM =="
# Unbind migrator from chassis-3
m_as ovn-chassis-3 ovs-vsctl -- set Interface migrator-p external_ids:iface-id=foo

check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1,ovn-chassis-3
sleep 1
m_as ovn-chassis-3 ovs-vsctl -- set Interface migrator-p external_ids:iface-id=migrator

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping mig-cont ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont "potential-duplicates"
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator "potential-duplicates"
done

echo "== Finalizing migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-3

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator
done

# Remove iface-id from src after migration is completed
m_as ovn-chassis-1 ovs-vsctl -- remove Interface migrator-p external_ids iface-id
check multinode_nbctl --wait=sb sync

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping cont${i} ovn-chassis-${i} migrator
done

# Finally, remove interface from src after migration is completed
m_as ovn-chassis-1 ovs-vsctl -- del-port migrator-p
for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping cont${i} ovn-chassis-${i} migrator
done

# Also do some recomputes ...
AS_BOX([Recompute after removing interface])
for i in 1 2 3; do
    m_as ovn-chassis-${i} ovn-appctl -t ovn-controller recompute
done

check multinode_nbctl --wait=sb sync

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping cont${i} ovn-chassis-${i} migrator
done

m_as ovn-chassis-1 killall tcpdump
m_as ovn-chassis-2 killall tcpdump
m_as ovn-chassis-3 killall tcpdump

AT_CLEANUP

AT_SETUP([HA: Check for missing garp on leader when BFD goes back up])
# Network topology
#    ┌────────────────────────────────────────────────────────────────────────────────────────────────────────┐
#    │                                                                                                        │
#    │    ┌───────────────────┐    ┌───────────────────┐    ┌───────────────────┐    ┌───────────────────┐    │
#    │    │   ovn-chassis-1   │    │  ovn-gw-1         │    │  ovn-gw-2         │    │  ovn-chassis-2    │    │
#    │    └─────────┬─────────┘    └───────────────────┘    └───────────────────┘    └───────────────────┘    │
#    │    ┌─────────┴─────────┐                                                                               │
#    │    │       inside1     │                                                                               │
#    │    │   192.168.1.1/24  │                                                                               │
#    │    └─────────┬─────────┘                                                                               │
#    │    ┌─────────┴─────────┐                                                                               │
#    │    │       inside      │                                                                               │
#    │    └─────────┬─────────┘                                                                               │
#    │    ┌─────────┴─────────┐                                                                               │
#    │    │    192.168.1.254  │                                                                               │
#    │    │         R1        │                                                                               │
#    │    │    192.168.0.254  │                                                                               │
#    │    └─────────┬─────────┘                                                                               │
#    │              └------eth1---------------┬--------eth1-----------┐                                       │
#    │                             ┌──────────┴────────┐    ┌─────────┴─────────┐                             │
#    │                             │    192.168.1.254  │    │   192.168.1.254   │                             │
#    │                             │         R1        │    │         R1        │                             │
#    │                             │    192.168.0.254  │    │   192.168.0.254   │                             │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘                             │
#    │                                       │                        │              ┌───────────────────┐    │
#    │                             ┌─────────┴─────────┐    ┌─────────┴─────────┐    │    192.168.0.1    │    │
#    │                             │       outside     │    │       outside     │    │        ext1       │    │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘    └─────────┬─────────┘    │
#    │                             ┌─────────┴─────────┐    ┌─────────┴─────────┐    ┌─────────┴─────────┐    │
#    │                             │    ln-outside     │    │    ln-outside     │    │       ln-ext1     │    │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘    └─────────┬─────────┘    │
#    │                             ┌─────────┴─────────┐    ┌─────────┴─────────┐    ┌─────────┴─────────┐    │
#    │                             │       br-ex       │    │       br-ex       │    │       br-ex       │    │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘    └─────────┬─────────┘    │
#    │                                       └---------eth2-----------┴-------eth2-------------┘              │
#    │                                                                                                        │
#    └────────────────────────────────────────────────────────────────────────────────────────────────────────┘

# The goal of this test is the check that GARP are properly generated by higest priority traffic when
# BFD goes down, and back up, and this whether the BFD event is due either to some bfd packet lost
# or by gw death.
# gw1 is the highest priority gw; gw2 the second priority and gw3 is configured as the lowest priority gw.
# So gw3 should in this test neither send garp or receive packets.
#
# Enable vconn so we can check the GARP from a log perspective.
m_as ovn-gw-1 ovn-appctl vlog/set vconn:dbg
m_as ovn-gw-2 ovn-appctl vlog/set vconn:dbg
m_as ovn-gw-3 ovn-appctl vlog/set vconn:dbg
m_as ovn-gw-1 ovn-appctl vlog/disable-rate-limit
m_as ovn-gw-2 ovn-appctl vlog/disable-rate-limit
m_as ovn-gw-3 ovn-appctl vlog/disable-rate-limit

# Add some logs in case the test fails.
export test_success=0
for chassis in ovn-gw-1 ovn-gw-2 ovn-chassis-1 ovn-chassis-2; do
    on_exit "if test $test_success != 1; then m_as $chassis ovs-vsctl list Interface > interfaces-${chassis}.txt; fi"
    on_exit "if test $test_success != 1; then m_as $chassis ovs-vsctl show > ovs-${chassis}.txt; fi"
    on_exit "if test $test_success != 1; then m_as $chassis ovs-ofctl dump-flows br-int > flow-${chassis}.txt; fi"
    on_exit "if test $test_success != 1; then m_as $chassis ovs-vsctl get open . external_ids > extids-${chassis}.txt; fi"
done

check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

ip_ch1=$(m_as ovn-chassis-1 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)
ip_gw1=$(m_as ovn-gw-1 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)
ip_gw2=$(m_as ovn-gw-2 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)
ip_gw3=$(m_as ovn-gw-3 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)

from_gw1_to_gw2=$(m_as ovn-gw-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw2)
from_gw1_to_gw3=$(m_as ovn-gw-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw3)
from_gw1_to_ch1=$(m_as ovn-gw-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_ch1)
from_gw2_to_gw1=$(m_as ovn-gw-2 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw1)
from_gw2_to_gw3=$(m_as ovn-gw-2 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw3)
from_gw2_to_ch1=$(m_as ovn-gw-2 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_ch1)
from_ch1_to_gw1=$(m_as ovn-chassis-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw1)
from_ch1_to_gw2=$(m_as ovn-chassis-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw2)

m_as ovn-chassis-1 ip link del hv1-vif1-p
m_as ovn-chassis-2 ip link del ext1-p

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-3 ip link show | grep -q genev_sys])

check multinode_nbctl ls-add inside
check multinode_nbctl ls-add outside
check multinode_nbctl ls-add ext
check multinode_nbctl lsp-add inside inside1 -- lsp-set-addresses inside1 "f0:00:c0:a8:01:01 192.168.1.1"
check multinode_nbctl lsp-add ext ext1 -- lsp-set-addresses ext1 "00:00:c0:a8:00:01 192.168.0.1"

multinode_nbctl create Logical_Router name=R1
check multinode_nbctl lrp-add R1 R1_inside f0:00:c0:a8:01:fe 192.168.1.254/24
check multinode_nbctl -- lsp-add inside inside_R1 \
                      -- set Logical_Switch_Port inside_R1 type=router options:router-port=R1_inside \
                      -- lsp-set-addresses inside_R1 router

check multinode_nbctl lrp-add R1 R1_outside f0:00:c0:a8:00:fe 192.168.0.254/24
check multinode_nbctl -- lsp-add outside outside_R1 \
                      -- set Logical_Switch_Port outside_R1 type=router options:router-port=R1_outside \
                      -- lsp-set-addresses outside_R1 router

multinode_nbctl -- --id=@gc0 create Gateway_Chassis name=outside_gw1 chassis_name=ovn-gw-1 priority=20 \
                -- --id=@gc1 create Gateway_Chassis name=outside_gw2 chassis_name=ovn-gw-2 priority=10 \
                -- --id=@gc2 create Gateway_Chassis name=outside_gw3 chassis_name=ovn-gw-3 priority=5 \
                -- set Logical_Router_Port R1_outside 'gateway_chassis=[@gc0,@gc1,@gc2]'

# Create localnet port in outside
check multinode_nbctl lsp-add-localnet-port outside ln-outside public

# Create localnet port in ext1
check multinode_nbctl lsp-add-localnet-port ext ln-ext1 public

# Make sure garp-max-timeout-sec is not set
m_as ovn-gw-1 ovs-vsctl remove open . external_ids garp-max-timeout-sec
m_as ovn-gw-2 ovs-vsctl remove open . external_ids garp-max-timeout-sec
m_as ovn-gw-3 ovs-vsctl remove open . external_ids garp-max-timeout-sec

m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-gw-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-gw-3 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_as ovn-chassis-1 /data/create_fake_vm.sh inside1 hv1-vif1 f0:00:c0:a8:01:01 192.168.1.1 24 192.168.1.254 2000::1/64 2000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh ext1 ext1 00:00:c0:a8:00:01 192.168.0.1 24 192.168.0.254 1000::3/64 1000::a

# There should be one ha_chassis_group with the name "R1_outside"
m_check_row_count HA_Chassis_Group 1 name=R1_outside

# There should be 2 ha_chassis rows in SB DB.
m_check_row_count HA_Chassis 3 'chassis!=[[]]'

ha_ch=$(m_fetch_column HA_Chassis_Group ha_chassis)
m_check_column "$ha_ch" HA_Chassis _uuid

gw1_chassis=$(m_fetch_column Chassis _uuid name=ovn-gw-1)
gw2_chassis=$(m_fetch_column Chassis _uuid name=ovn-gw-2)
gw3_chassis=$(m_fetch_column Chassis _uuid name=ovn-gw-3)

wait_bfd_enabled() {
    chassis=$1
    interface=$2
    echo "Using $chassis $interface"
    OVS_WAIT_UNTIL([test 1 = $(m_as $chassis ovs-vsctl --bare --columns bfd find Interface name=$interface | \
grep "enable=true" | wc -l)
])
}

wait_bfd_up() {
    hv1=$1
    hv2=$2

    echo "checking bfd_status for $hv1 => $hv2"
    OVS_WAIT_UNTIL([
        state=$(m_as $hv1 ovs-vsctl get interface $hv2 bfd_status:state)
        remote_state=$(m_as $hv1 ovs-vsctl get interface $hv2 bfd_status:remote_state)
        echo "$(date +%H:%M:%S.%03N) bfd state = $state, remote_state = $remote_state"
        test "$state" = "up" -a "$remote_state" = "up"
    ])
}

# check BFD enablement on tunnel ports from ovn-gw-1 ##########
for chassis in $from_gw1_to_gw2 $from_gw1_to_gw3 $from_gw1_to_ch1; do
    echo "checking ovn-gw-1 -> $chassis"
    wait_bfd_enabled ovn-gw-1 $chassis
done

# check BFD enablement on tunnel ports from ovn-gw-2 ##########
for chassis in $from_gw2_to_gw1 $from_gw2_to_gw3 $from_gw2_to_ch1; do
    echo "checking ovn-gw-2 -> $chassis"
    wait_bfd_enabled ovn-gw-2 $chassis
done

# check BFD enablement on tunnel ports from ovn-chassis-1 ###########
for chassis in $from_ch1_to_gw1 $from_ch1_to_gw2; do
    echo "checking ovn-chassis-1 -> $chassis"
    wait_bfd_enabled ovn-chassis-1 $chassis
done

# Make sure there is no nft table left. Do not use nft directly as might not be installed in container.
gw1_pid=$(podman inspect -f '{{.State.Pid}}' ovn-gw-1)
nsenter --net=/proc/$gw1_pid/ns/net nft list tables | grep ovn-test && nsenter --net=/proc/$gw1_pid/ns/net nft delete table ip ovn-test
on_exit "nsenter --net=/proc/$gw1_pid/ns/net nft list tables | grep ovn-test && nsenter --net=/proc/$gw1_pid/ns/net nft delete table ip ovn-test"

for chassis in $from_gw1_to_gw2 $from_gw1_to_gw3 $from_gw1_to_ch1; do
    wait_bfd_up ovn-gw-1 $chassis
done
for chassis in $from_gw2_to_gw1 $from_gw2_to_gw3 $from_gw2_to_ch1; do
    wait_bfd_up ovn-gw-2 $chassis
done
for chassis in $from_ch1_to_gw1 $from_ch1_to_gw2; do
    wait_bfd_up ovn-chassis-1 $chassis
done

m_wait_row_count Port_Binding 1 logical_port=cr-R1_outside chassis=$gw1_chassis
check multinode_nbctl --wait=hv sync

start_tcpdump() {
    echo "$(date +%H:%M:%S.%03N) Starting tcpdump"
    M_START_TCPDUMP([ovn-chassis-1], [-neei hv1-vif1-p], [ch1])
    M_START_TCPDUMP([ovn-chassis-2], [-neei eth2], [ch2])
    M_START_TCPDUMP([ovn-gw-1], [-neei eth2], [gw1])
    M_START_TCPDUMP([ovn-gw-1], [-neei eth2 -Q out], [gw1_out])
    M_START_TCPDUMP([ovn-gw-2], [-neei eth2], [gw2])
    M_START_TCPDUMP([ovn-gw-2], [-neei eth2 -Q out], [gw2_out])
    M_START_TCPDUMP([ovn-gw-3], [-neei eth2], [gw3])
    M_START_TCPDUMP([ovn-gw-3], [-neei eth2 -Q out], [gw3_out])
}

stop_tcpdump() {
    echo "$(date +%H:%M:%S.%03N) Stopping tcpdump"
    m_kill 'ovn-gw-1 ovn-gw-2 ovn-gw-3 ovn-chassis-1 ovn-chassis-2' tcpdump
}

# Send packets from chassis2 (ext1) to chassis1
send_background_packets() {
    echo "$(date +%H:%M:%S.%03N) Sending packets in Background"
    start_tcpdump
    M_NS_DAEMONIZE([ovn-chassis-2], [ext1], [ping -f -i 0.1 192.168.1.1], [ping.pid])
}

stop_sending_background_packets() {
    echo "$(date +%H:%M:%S.%03N) Stopping Background process"
    m_as ovn-chassis-1 ps -ef | grep -v grep | grep -q ping && \
        m_as ovn-chassis-1 echo "Stopping ping on ovn-chassis-1" && killall ping
    m_as ovn-chassis-2 ps -ef | grep -v grep | grep -q ping && \
        m_as ovn-chassis-2 echo "Stopping ping on ovn-chassis-2" && killall ping
    stop_tcpdump
}

check_for_new_garps() {
    hv=$1
    expecting_garp=$2
    n_new_garps=$(cat ${hv}_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")

    if [ "$expecting_garp" == "true" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting/checking for garp from $hv - Starting with $n_new_garps])
        OVS_WAIT_UNTIL([
            n_garps=$n_new_garps
            n_new_garps=$(cat ${hv}_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
            echo "We saw $n_new_garps so far on ${hv}."
            test "$n_garps" -ne "$n_new_garps"
        ])
    else
        AS_BOX([$(date +%H:%M:%S.%03N) Checking no garp from ${hv}])
        # Waiting a few seconds to get a chance to see unexpected garps.
        sleep 3
        n_garps=$(cat ${hv}_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
        AT_CHECK([test "$n_garps" -eq "$n_new_garps"])
    fi
}

check_for_new_echo_pkts() {
    hv=$1
    mac_src=$2
    mac_dst=$3
    expecting_pkts=$4
    n_new_echo_req=$(cat ${hv}.tcpdump | grep -c "$mac_src > $mac_dst, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo request")
    n_new_echo_rep=$(cat ${hv}.tcpdump | grep -c "$mac_dst > $mac_src, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo reply")

    if [ "$expecting_pkts" == "true" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting/checking for echo pkts through ${hv}])
        echo "Starting with $n_new_echo_req requests and $n_new_echo_rep replies so far on ${hv}."
        OVS_WAIT_UNTIL([
            n_echo_req=$n_new_echo_req
            n_echo_rep=$n_new_echo_rep
            n_new_echo_req=$(cat ${hv}.tcpdump | grep -c "$mac_src > $mac_dst, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo request")
            n_new_echo_rep=$(cat ${hv}.tcpdump | grep -c "$mac_dst > $mac_src, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo reply")
            echo "We saw $n_new_echo_req requests and $n_new_echo_rep replies so far on ${hv}."
            test "$n_echo_req" -ne "$n_new_echo_req" && test "$n_echo_rep" -ne "$n_new_echo_rep"
        ])
    else
        AS_BOX([$(date +%H:%M:%S.%03N) Checking no pkts from ${hv}])
        # Waiting a few seconds to get a chance to see unexpected pkts.
        sleep 3
        n_echo_req=$(cat ${hv}.tcpdump | grep -c "$mac_src > $mac_dst, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo request")
        n_echo_rep=$(cat ${hv}.tcpdump | grep -c "$mac_dst > $mac_src, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo reply")
        echo "We saw $n_new_echo_req requests and $n_new_echo_rep replies on ${hv}."
        AT_CHECK([test "$n_echo_req" -eq "$n_new_echo_req" && test "$n_echo_rep" -eq "$n_new_echo_rep"])
    fi
}

dump_statistics() {
    n1=$(m_as ovn-gw-1 grep -c Changing /var/log/ovn/ovn-controller.log)
    n2=$(m_as ovn-gw-2 grep -c Changing /var/log/ovn/ovn-controller.log)
    n3=$(m_as ovn-gw-3 grep -c Changing /var/log/ovn/ovn-controller.log)
    ch1_req=$(grep -c "ICMP echo request" ch1.tcpdump)
    ch1_rep=$(grep -c "ICMP echo reply" ch1.tcpdump)
    ch2_req=$(grep -c "ICMP echo request" ch2.tcpdump)
    ch2_rep=$(grep -c "ICMP echo reply" ch2.tcpdump)
    gw1_req=$(grep -c "ICMP echo request" gw1.tcpdump)
    gw1_rep=$(grep -c "ICMP echo reply" gw1.tcpdump)
    gw2_req=$(grep -c "ICMP echo request" gw2.tcpdump)
    gw2_rep=$(grep -c "ICMP echo reply" gw2.tcpdump)
    gw3_req=$(grep -c "ICMP echo request" gw3.tcpdump)
    gw3_rep=$(grep -c "ICMP echo reply" gw3.tcpdump)
    echo "$n1 claims in gw1, $n2 in gw2 and $n3 on gw3"
    echo "ch2_request=$ch2_req gw1_request=$gw1_req gw2_request=$gw2_req gw3_request=$gw3_req ch1_request=$ch1_req ch1_reply=$ch1_rep gw1_reply=$gw1_rep gw2_reply=$gw2_rep gw3_reply=$gw3_rep ch2_reply=$ch2_rep"
}

check_migration_between_gw1_and_gw2() {
    action=$1
    send_background_packets

    # We make sure gw1 is leader since enough time that it generated all its garps.
    AS_BOX([$(date +%H:%M:%S.%03N) Waiting all garps sent by gw1])
    n_new_garps=$(cat gw1_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
    OVS_WAIT_UNTIL([
        n_garps=$n_new_garps
        echo "We saw $n_garps so far."
        # Garp delay might be up to 8 seconds.
        sleep 10
        n_new_garps=$(cat gw1_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
        test "$n_garps" -eq "$n_new_garps"
    ])

    # All packets should go through gw1, and none through gw2 or gw3.
    check_for_new_echo_pkts gw1 "00:00:c0:a8:00:01" "f0:00:c0:a8:00:fe" "true"
    check_for_new_echo_pkts gw2 "00:00:c0:a8:00:01" "f0:00:c0:a8:00:fe" "false"
    check_for_new_echo_pkts gw3 "00:00:c0:a8:00:01" "f0:00:c0:a8:00:fe" "false"

    flap_count_gw_1=$(m_as ovn-gw-1 ovs-vsctl get interface $from_gw1_to_gw2 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')
    flap_count_gw_2=$(m_as ovn-gw-2 ovs-vsctl get interface $from_gw2_to_gw1 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')

    if [ test "$action" == "stop_bfd" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Blocking bfd on gw1 (from $ip_gw1 to $ip_gw2)])
        nsenter --net=/proc/$gw1_pid/ns/net nft add table ip ovn-test
        nsenter --net=/proc/$gw1_pid/ns/net nft 'add chain ip ovn-test INPUT { type filter hook input priority 0; policy accept; }'
        # Drop BFD from gw-1 to gw-2: geneve port (6081), inner port 3784 (0xec8), Session state Up, Init, Down.
        nsenter --net=/proc/$gw1_pid/ns/net nft add rule ip ovn-test INPUT ip daddr $ip_gw1 ip saddr $ip_gw2 udp dport 6081 '@th,416,16 == 0x0ec8 @th,472,8 == 0xc0  counter drop'
        nsenter --net=/proc/$gw1_pid/ns/net nft add rule ip ovn-test INPUT ip daddr $ip_gw1 ip saddr $ip_gw2 udp dport 6081 '@th,416,16 == 0x0ec8 @th,472,8 == 0x80  counter drop'
        nsenter --net=/proc/$gw1_pid/ns/net nft add rule ip ovn-test INPUT ip daddr $ip_gw1 ip saddr $ip_gw2 udp dport 6081 '@th,416,16 == 0x0ec8 @th,472,8 == 0x40  counter drop'

        # We do not check that packets go through gw2 as BFD between chassis-2 and gw1 is still up
    fi

    if [ test "$action" == "kill_gw2" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Killing gw2 ovn-controller])
        on_exit 'm_as ovn-gw-2 /usr/share/openvswitch/scripts/ovs-ctl status ||
                 m_as ovn-gw-2 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-1'
        on_exit 'm_as ovn-gw-2 /usr/share/ovn/scripts/ovn-ctl status_controller ||
                 m_as ovn-gw-2 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}'

        m_as ovn-gw-2 kill -9 $(m_as ovn-gw-2 cat /run/ovn/ovn-controller.pid)
        m_as ovn-gw-2 kill -9 $(m_as ovn-gw-2 cat /run/openvswitch/ovs-vswitchd.pid)
        m_as ovn-gw-2 kill -9 $(m_as ovn-gw-2 cat /run/openvswitch/ovsdb-server.pid)
        # Also delete datapath (flows)
        m_as ovn-gw-2 ovs-dpctl del-dp system@ovs-system
    fi

    if [ test "$action" == "kill_gw1" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Killing gw1 ovn-controller])
        on_exit 'm_as ovn-gw-1 /usr/share/openvswitch/scripts/ovs-ctl status ||
                 m_as ovn-gw-1 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-1'
        on_exit 'm_as ovn-gw-1 /usr/share/ovn/scripts/ovn-ctl status_controller ||
                 m_as ovn-gw-1 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}'

        m_as ovn-gw-1 kill -9 $(m_as ovn-gw-1 cat /run/ovn/ovn-controller.pid)
        m_as ovn-gw-1 kill -9 $(m_as ovn-gw-1 cat /run/openvswitch/ovs-vswitchd.pid)
        m_as ovn-gw-1 kill -9 $(m_as ovn-gw-1 cat /run/openvswitch/ovsdb-server.pid)
        # Also delete datapath (flows)
        m_as ovn-gw-1 ovs-dpctl del-dp system@ovs-system
    fi

    if [ test "$action" == "kill_gw2" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting for flap count between gw1 and gw2 to increase])
        OVS_WAIT_UNTIL([
            new_flap_count=$(m_as ovn-gw-1 ovs-vsctl get interfac $from_gw1_to_gw2 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')
            echo "Comparing $new_flap_count versus $flap_count_gw_1"
            test "$new_flap_count" -gt "$((flap_count_gw_1))"
        ])
    else
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting for flap count between gw2 and gw1 to increase])
        OVS_WAIT_UNTIL([
            new_flap_count=$(m_as ovn-gw-2 ovs-vsctl get interfac $from_gw2_to_gw1 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')
            echo "Comparing $new_flap_count versus $flap_count_gw_2"
            test "$new_flap_count" -gt "$((flap_count_gw_2))"
        ])

    fi
    AS_BOX([$(date +%H:%M:%S.%03N) Flapped!])

    # Wait a few more second for the fight.
    sleep 2
    AS_BOX([$(date +%H:%M:%S.%03N) Statistics after flapping])
    dump_statistics

    if [ test "$action" == "stop_bfd" ]; then
        # gw1 still alive and gw2 tried to claim => gw1 should restart generating garps.
        check_for_new_garps gw1 "true"
        check_for_new_garps gw2 "false"
        check_for_new_garps gw3 "false"
        check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
        check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
        AS_BOX([$(date +%H:%M:%S.%03N) Unblocking bfd on gw1])
        nsenter --net=/proc/$gw1_pid/ns/net nft -a list ruleset
        nsenter --net=/proc/$gw1_pid/ns/net nft delete table ip ovn-test
    fi

    if [ test "$action" == "kill_gw2" ]; then
        # gw1 still alive, but gw2 did not try to claim => gw1 should not generate new garps.
        check_for_new_garps gw1 "false"
        check_for_new_garps gw2 "false"
        check_for_new_garps gw3 "false"
        check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
        check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw2 ovn-vswitchd])
        m_as ovn-gw-2 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-2

        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw2 ovn-controller])
        m_as ovn-gw-2 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}
    fi

    if [ test "$action" == "kill_gw1" ]; then
        # gw1 died => gw2 should generate garps.
        check_for_new_garps gw1 "false"
        check_for_new_garps gw2 "true"
        check_for_new_garps gw3 "false"
        check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
        check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw1 ovn-vswitchd])
        m_as ovn-gw-1 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-1

        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw1 ovn-controller])
        m_as ovn-gw-1 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}
    fi

    # The network is now restored => packets should go through gw1 and reach chassis-1.
    check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
    check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
    check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
    check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
    AS_BOX([$(date +%H:%M:%S.%03N) Statistics after network restored])
    dump_statistics
    stop_sending_background_packets
}

start_tcpdump
AS_BOX([$(date +%H:%M:%S.%03N) Sending packet from hv1-vif1(inside1) to ext1])
M_NS_CHECK_EXEC([ovn-chassis-1], [hv1-vif1], [ping -c3 -q -i 0.1 192.168.0.1 | FORMAT_PING],
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
stop_tcpdump

# It should have gone through gw1 and not gw2
AS_BOX([$(date +%H:%M:%S.%03N) Checking it went through gw1 and not gw2])
AT_CHECK([cat gw2.tcpdump | grep "ICMP echo"], [1], [dnl
])

AT_CHECK([cat gw1.tcpdump | grep "ICMP echo" | cut -d  ' ' -f2-15], [0], [dnl
f0:00:c0:a8:00:fe > 00:00:c0:a8:00:01, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo request,
00:00:c0:a8:00:01 > f0:00:c0:a8:00:fe, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo reply,
f0:00:c0:a8:00:fe > 00:00:c0:a8:00:01, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo request,
00:00:c0:a8:00:01 > f0:00:c0:a8:00:fe, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo reply,
f0:00:c0:a8:00:fe > 00:00:c0:a8:00:01, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo request,
00:00:c0:a8:00:01 > f0:00:c0:a8:00:fe, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo reply,
])

# We stop bfd between gw1 & gw2, but keep gw1 & gw2 running.
check_migration_between_gw1_and_gw2 "stop_bfd"

# We simulate death of gw2. It should not have any effect.
check_migration_between_gw1_and_gw2 "kill_gw2"

# We simulate death of gw1. gw2 should take over.
check_migration_between_gw1_and_gw2 "kill_gw1"

export test_success=1
AT_CLEANUP
])

AT_SETUP([redirect-bridged to non-gw destination switch port])

check_fake_multinode_setup
cleanup_multinode_resources
# This test uses the following logical network:
#
# +-----------+
# | ls-public |
# +-----------+
#       |
#       |
# +-----------+
# | gw-router |
# +-----------+
#       |
#       |
# +-----------+
# | ls-local  |
# +-----------+
#
# The router port from gw-router to ls-public is a distributed gateway port.
# It also has options:redirect-type=bridged set.
#
# We create vm1 attached to ls-local that is bound to ovn-chassis-1. We
# also create vm2 attached to ls-public that is bound to ovn-chassis-1.
# The DGP on gw-router is bound to ovn-gw-1.
#
# Our goal is to successfully ping from vm1 to vm2. In order for this to
# work, the ping will have to traverse gw-router. vm1 and vm2 are bound to
# the same chassis, but the DGP is bound to ovn-gw-1. We therefore expect
# the following:
#
# The ping starts by entering ls-local on ovn-chassis-1.
# The ping then goes through the ingress pipeline of gw-router on ovn-chassis-1.
# The ping then goes through ls-public's localnet port to reach ovn-gw-1.
# The ping's destination MAC should be the DGP's MAC. So the ping will
# get processed first by ls-public on ovn-gw-1, then will be redirected to
# gw-router on ovn-gw-1. The packet will then re-enter ls-public on ovn-gw-1.
# The ping will then get redirected over the localnet back to ovn-chassis-1.
# From here, the ls-public pipeline can run and the ping will be output to vm2.
#

check multinode_nbctl ls-add ls-local
check multinode_nbctl lsp-add ls-local vm1
check multinode_nbctl lsp-set-addresses vm1 "00:00:00:00:01:02 10.0.0.2 10::2"

check multinode_nbctl ls-add ls-public
check multinode_nbctl lsp-add ls-public vm2
check multinode_nbctl lsp-set-addresses vm2 "00:00:00:00:02:02 20.0.0.2 20::2"

check multinode_nbctl lsp-add-localnet-port ls-public ln-public public

check multinode_nbctl lr-add gw-router

check multinode_nbctl lrp-add gw-router ro-local 00:00:00:00:01:01 10.0.0.1/8 10::1/64
check multinode_nbctl lsp-add-router-port ls-local local-ro ro-local

check multinode_nbctl lrp-add gw-router ro-public 00:00:00:00:02:01 20.0.0.1/8 20::1/64
check multinode_nbctl lsp-add-router-port ls-public public-ro ro-public

check multinode_nbctl lrp-set-gateway-chassis ro-public ovn-gw-1
check multinode_nbctl lrp-set-redirect-type ro-public bridged

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-chassis-mac-mappings="public:aa:bb:cc:dd:01:01"
m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-chassis-mac-mappings="public:aa:bb:cc:dd:02:01"

m_as ovn-chassis-1 /data/create_fake_vm.sh vm1 vm1 00:00:00:00:01:02 1342 10.0.0.2 8 10.0.0.1 10::2/64 10::1
m_as ovn-chassis-1 /data/create_fake_vm.sh vm2 vm2 00:00:00:00:02:02 1342 20.0.0.2 8 20.0.0.1 20::2/64 20::1

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [vm1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CLEANUP

AT_SETUP([IPv6 NA received on non resident chassis])
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del ls1p1-p
m_as ovn-chassis-1 ip link del ls1p2-p
m_as ovn-chassis-1 ip link del ls2p1-p

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])

check multinode_nbctl ls-add ls1
check multinode_nbctl lsp-add ls1 ls1p1
check multinode_nbctl lsp-set-addresses ls1p1 "00:00:00:01:01:02 192.168.1.1 2001::1"
check multinode_nbctl lsp-add ls1 ls1p2
check multinode_nbctl lsp-set-addresses ls1p2 "00:00:00:01:02:02 192.168.1.2 2001::2"
check multinode_nbctl lr-add lr1
check multinode_nbctl lrp-add lr1 lr1-ls1 00:00:00:00:00:01 192.168.1.254/24 2001::a/64
check multinode_nbctl lsp-add-router-port ls1 ls1-lr1 lr1-ls1

check multinode_nbctl lrp-add lr1 lr1-ls2 00:00:00:00:00:02 192.168.2.254/24 2002::a/64
check multinode_nbctl ls-add ls2
check multinode_nbctl lsp-add-router-port ls2 ls2-lr1 lr1-ls2
check multinode_nbctl lsp-add ls2 ls2p1
check multinode_nbctl lsp-set-addresses ls2p1 "00:00:00:02:01:02 192.168.2.1 2002::1"

check multinode_nbctl lsp-add ls1 ls1p3
check multinode_nbctl lsp-set-addresses ls1p3 "00:00:00:01:03:02 192.168.1.3 2001::3"
check multinode_nbctl lrp-add lr1 lr1-pub 0a:0a:56:33:02:ff 172.18.86.254/24 6812:86::254/64
check multinode_nbctl ls-add pub                    \
    -- lsp-add-router-port pub pub-lr1 lr1-pub       \
    -- lsp-add-localnet-port pub pub-ln public

check multinode_nbctl lrp-set-gateway-chassis lr1-pub ovn-chassis-2

check multinode_nbctl lr-nat-add lr1 dnat_and_snat 172.18.86.11 192.168.1.1 ls1p1 0a:0a:56:33:02:11
check multinode_nbctl lr-nat-add lr1 dnat_and_snat 6812:86::11 2001::1 ls1p1 0a:0a:56:33:02:11

m_as ovn-chassis-1 /data/create_fake_vm.sh ls1p1 ls1p1 00:00:00:01:01:02 1500 192.168.1.1 24 192.168.1.254 2001::1/64 2001::a
m_as ovn-chassis-1 /data/create_fake_vm.sh ls1p2 ls1p2 00:00:00:01:02:02 1500 192.168.1.2 24 192.168.1.254 2001::2/64 2001::a
m_as ovn-chassis-1 /data/create_fake_vm.sh ls2p1 ls2p1 00:00:00:02:01:02 1500 192.168.2.1 24 192.168.2.254 2002::2/64 2002::a
m_add_internal_port ovn-chassis-1 ovn-ext1 br-ex ext1 172.18.86.101/24 "" 6812:86::101/64
m_add_internal_port ovn-chassis-2 ovn-ext2 br-ex ext2 172.18.86.102/24 "" 6812:86::102/64

m_central_as ovn-sbctl --all destroy  mac_binding

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping -q -c 3 -i 0.3 -w 2 172.18.86.101 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping -q -c 3 -i 0.3 -w 2 172.18.86.102 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.102" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::101 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::102 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:102" logical_port="lr1-pub"

# We remove all mac_bindings to check that neighbor learning works also in the other direction.
m_central_as ovn-sbctl --all destroy  mac_binding

# Also remove addresses learned on ovn-ext
M_NS_CHECK_EXEC([ovn-chassis-1], [ovn-ext1], [ip -6 neigh del 6812:86::11 dev ext1 lladdr 0a:0a:56:33:02:11])
M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ip -6 neigh del 6812:86::11 dev ext2 lladdr 0a:0a:56:33:02:11])

M_NS_CHECK_EXEC([ovn-chassis-1], [ovn-ext1], [ping -q -c 3 -i 0.3 -w 2 172.18.86.11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ping -q -c 3 -i 0.3 -w 2 172.18.86.11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ovn-ext1], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:102" logical_port="lr1-pub"

# Ping from ext2 while no mac_binding yet.
m_central_as ovn-sbctl --all destroy  mac_binding
M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ip -6 neigh del 6812:86::11 dev ext2 lladdr 0a:0a:56:33:02:11])

M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:102" logical_port="lr1-pub"

AT_CLEANUP
